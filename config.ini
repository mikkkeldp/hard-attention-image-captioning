[vocab]
;path to file that contains captions
caption_path = ./dataset/Flickr8k.token.txt  
;path to the training set image names
train_path = ./dataset/Flickr_8k.trainImages.txt 
;path to where vocab .pkl file is to be saved
vocab_path = ./data/vocab.pkl 
;minimum word frequency threshold
threshold = 1

[train]
;path for saving trained models
model_path = ./trained_models/
;path for vocabulary wrapper
vocab_path = ./data/vocab.pkl
;directory for images
image_dir = ./dataset/Flickr8k_Dataset
;path for caption file
caption_path = ./dataset/Flickr8k.token.txt
;path for train split file'
train_path = ./dataset/Flickr_8k.trainImages.txt
;input image size for particular pretrained CNN (244/244/299)
image_size = 299
;step size for printing log info
log_step = 20
;step size for saving trained models
save_step = 1000

;cnn used for feature extraction
cnn = inception
;dimension of extracted features (1024, 512,192)
encoder_size = 768
;dimension of word embedding vectors
embed_size = 200
;dimension of encoded image
encoded_image_size = 14
;dimension of attention layers
attention_size = 384
;dimension of lstm hidden states
hidden_size = 384
;use pre-trained embedding layer 
glove = False

;number of epochs 
num_epochs = 10
;batch size 
batch_size = 32
;number of parallel workers during training
num_workers = 4
;learning rate for optimizer
learning_rate = 0.0005

[eval]
;path for trained encoder
encoder_path = ./trained_models/encoder-9-496.ckpt
;path for trained decoder
decoder_path = ./trained_models/decoder-9-496.ckpt
;path to vocabulary wrapper
vocab_path = ./data/vocab.pkl
;path to dataset images
image_dir = ./dataset/Flickr8k_Dataset
;path to file that contains captions
caption_path = ./dataset/Flickr8k.token.txt
;path for val split file
val_path = ./dataset/Flickr_8k.devImages.txt
;input image size for particular pretrained CNN
image_size = 224

;cnn used for feature extraction (reset, vgg)
cnn = resnet
;dimension of extracted features (1024, 512)
encoder_size = 1024
;dimension of word embedding vectors
embed_size = 200
;dimension of encoded image
encoded_image_size = 14
;dimension of attention layers
attention_size = 384
;use pre-trained embedding layer 
glove = True
;dimension of lstm hidden states
hidden_size = 384
;batch size 
batch_size = 1
;number of parallel workers during training
num_workers = 0


; Show attend and tell model parameters
; hidden size: 1024
; embed_size = 512
; epochs = 10
; batch_size = 100
; lr = 0.01
; optimizer adam

